{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ex3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGl0RqrEGUPt"
      },
      "source": [
        "# Exercise 3 Names and IDs\n",
        " 1. Tomer Lev-Lehman 203458674\n",
        " 2. Yiftach Savransky 312141369"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETIY8hrPEkXw"
      },
      "source": [
        "#dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "# mount google drive\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import preprocessing\n",
        "from  sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import make_scorer, roc_auc_score\n",
        "\n",
        "from keras.metrics import AUC\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "from collections import Counter \n",
        "from imblearn.pipeline import make_pipeline, Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics import confusion_matrix,plot_confusion_matrix, ConfusionMatrixDisplay\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU89tcUHEOXF"
      },
      "source": [
        "# **Load data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWD1XsseD8_O"
      },
      "source": [
        "# path to csv dataset in mounted drive\n",
        "orig_url='https://drive.google.com/file/d/1_w4YVAAxHfJ0h_-4z0w4Mk5JkhbjFn2c/view?usp=sharing'\n",
        "\n",
        "file_id = orig_url.split('/')[-2]\n",
        "dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
        "url = requests.get(dwn_url).text\n",
        "csv_raw = StringIO(url)\n",
        "data_frame = pd.read_csv(csv_raw)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br8Sdt3lESYV"
      },
      "source": [
        "# **Preprocess**\n",
        "\n",
        "We preprocess the data in two main stages: encoding and normalizing\n",
        "\n",
        "\n",
        "\n",
        "**1.   Encoding**\n",
        "\n",
        "Every binary feature we convert to $\\{0,1\\}$.\n",
        "\n",
        "Every Multi-value categorical feature we encode using one hot encoder and dummy variables.\n",
        "\n",
        "**2.   Normalization**\n",
        "\n",
        "Every numeric feature is normalized using min-max normaliztion to the range: $[0,1]$\n",
        "\n",
        "Those steps were taken to ensure that the feature are all within the same range.\n",
        "\n",
        "**Missing values** were also addressed in the code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "VZIra7TjHVzN",
        "outputId": "4056bf2b-49ba-43a8-c595-bacff24a6fa8"
      },
      "source": [
        "# data_frame.apply(lambda x: sum(x.isnull()),axis=0)\n",
        "# Missing values - thee only feature which has missing values is NEWSITEID - 465 missing. \n",
        "# we will regard the missing value - NA as another value.  \n",
        "processed_df = pd.DataFrame()\n",
        "#label encoding\n",
        "# Converting Categorical Variables\n",
        "le_features = [\"INTENSIVE\",\"INCLUSIONFRS\",\"NOAGENTS\", \"ASPIRIN\", \"SUB_CKD\", \"RACE_BLACK\", \"FEMALE\", \"SUB_CVD\",\"SUB_CLINICALCVD\",\"SUB_SUBCLINICALCVD\",\"SUB_SENIOR\",\"STATIN\",\"EVENT_PRIMARY\"]\n",
        "le = LabelEncoder()\n",
        "for i in le_features:\n",
        "    processed_df[i] = le.fit_transform(data_frame[i])\n",
        "\n",
        "#dummy variable - One Hot Encoder\n",
        "dummy_features = [\"NEWSITEID\", \"RACE4\"]\n",
        "\n",
        "for i in dummy_features:\n",
        "  processed_df = processed_df.join(pd.get_dummies(data_frame[i], prefix=i))\n",
        "\n",
        "#normalize\n",
        "norm_features = [\"RISK10YRS\", \"N_AGENTS\",\"SBP\", \"DBP\", \"EGFR\", \"SMOKE_3CAT\", \"SCREAT\", \"AGE\",\"CHR\",\"GLUR\",\"HDL\",\"TRR\",\"UMALCR\",\"BMI\",\"SBPTERTILE\"]\n",
        "\n",
        "for i in norm_features:\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  x = data_frame[i].values.reshape(-1, 1) #returns a numpy array\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  processed_df[i] = pd.DataFrame(x_scaled)\n",
        "\n",
        "display(processed_df)\n",
        "outcome = \"EVENT_PRIMARY\"\n",
        "predictors = list(processed_df.columns.values)\n",
        "predictors.remove(\"EVENT_PRIMARY\")\n",
        "print(f\"outcome: {outcome}\")\n",
        "print(f\"predictors: {predictors}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>INTENSIVE</th>\n",
              "      <th>INCLUSIONFRS</th>\n",
              "      <th>NOAGENTS</th>\n",
              "      <th>ASPIRIN</th>\n",
              "      <th>SUB_CKD</th>\n",
              "      <th>RACE_BLACK</th>\n",
              "      <th>FEMALE</th>\n",
              "      <th>SUB_CVD</th>\n",
              "      <th>SUB_CLINICALCVD</th>\n",
              "      <th>SUB_SUBCLINICALCVD</th>\n",
              "      <th>SUB_SENIOR</th>\n",
              "      <th>STATIN</th>\n",
              "      <th>EVENT_PRIMARY</th>\n",
              "      <th>NEWSITEID_1.0</th>\n",
              "      <th>NEWSITEID_2.0</th>\n",
              "      <th>NEWSITEID_3.0</th>\n",
              "      <th>NEWSITEID_4.0</th>\n",
              "      <th>NEWSITEID_5.0</th>\n",
              "      <th>NEWSITEID_6.0</th>\n",
              "      <th>NEWSITEID_8.0</th>\n",
              "      <th>NEWSITEID_9.0</th>\n",
              "      <th>NEWSITEID_10.0</th>\n",
              "      <th>NEWSITEID_11.0</th>\n",
              "      <th>NEWSITEID_12.0</th>\n",
              "      <th>NEWSITEID_13.0</th>\n",
              "      <th>NEWSITEID_15.0</th>\n",
              "      <th>NEWSITEID_18.0</th>\n",
              "      <th>NEWSITEID_19.0</th>\n",
              "      <th>NEWSITEID_20.0</th>\n",
              "      <th>NEWSITEID_21.0</th>\n",
              "      <th>NEWSITEID_22.0</th>\n",
              "      <th>NEWSITEID_23.0</th>\n",
              "      <th>NEWSITEID_24.0</th>\n",
              "      <th>NEWSITEID_25.0</th>\n",
              "      <th>NEWSITEID_27.0</th>\n",
              "      <th>NEWSITEID_29.0</th>\n",
              "      <th>NEWSITEID_31.0</th>\n",
              "      <th>NEWSITEID_32.0</th>\n",
              "      <th>NEWSITEID_33.0</th>\n",
              "      <th>NEWSITEID_36.0</th>\n",
              "      <th>...</th>\n",
              "      <th>NEWSITEID_77.0</th>\n",
              "      <th>NEWSITEID_79.0</th>\n",
              "      <th>NEWSITEID_80.0</th>\n",
              "      <th>NEWSITEID_81.0</th>\n",
              "      <th>NEWSITEID_82.0</th>\n",
              "      <th>NEWSITEID_83.0</th>\n",
              "      <th>NEWSITEID_85.0</th>\n",
              "      <th>NEWSITEID_87.0</th>\n",
              "      <th>NEWSITEID_88.0</th>\n",
              "      <th>NEWSITEID_89.0</th>\n",
              "      <th>NEWSITEID_90.0</th>\n",
              "      <th>NEWSITEID_91.0</th>\n",
              "      <th>NEWSITEID_92.0</th>\n",
              "      <th>NEWSITEID_94.0</th>\n",
              "      <th>NEWSITEID_95.0</th>\n",
              "      <th>NEWSITEID_96.0</th>\n",
              "      <th>NEWSITEID_97.0</th>\n",
              "      <th>NEWSITEID_98.0</th>\n",
              "      <th>NEWSITEID_99.0</th>\n",
              "      <th>NEWSITEID_100.0</th>\n",
              "      <th>NEWSITEID_102.0</th>\n",
              "      <th>RACE4_BLACK</th>\n",
              "      <th>RACE4_HISPANIC</th>\n",
              "      <th>RACE4_OTHER</th>\n",
              "      <th>RACE4_WHITE</th>\n",
              "      <th>RISK10YRS</th>\n",
              "      <th>N_AGENTS</th>\n",
              "      <th>SBP</th>\n",
              "      <th>DBP</th>\n",
              "      <th>EGFR</th>\n",
              "      <th>SMOKE_3CAT</th>\n",
              "      <th>SCREAT</th>\n",
              "      <th>AGE</th>\n",
              "      <th>CHR</th>\n",
              "      <th>GLUR</th>\n",
              "      <th>HDL</th>\n",
              "      <th>TRR</th>\n",
              "      <th>UMALCR</th>\n",
              "      <th>BMI</th>\n",
              "      <th>SBPTERTILE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.351322</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.459119</td>\n",
              "      <td>0.425532</td>\n",
              "      <td>0.309159</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.190608</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.229508</td>\n",
              "      <td>0.152893</td>\n",
              "      <td>0.131944</td>\n",
              "      <td>0.028858</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.346973</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.350636</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.415094</td>\n",
              "      <td>0.329787</td>\n",
              "      <td>0.268058</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.207182</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.469945</td>\n",
              "      <td>0.260331</td>\n",
              "      <td>0.305556</td>\n",
              "      <td>0.069009</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.270478</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.196410</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.446541</td>\n",
              "      <td>0.553191</td>\n",
              "      <td>0.313531</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.185083</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.297814</td>\n",
              "      <td>0.297521</td>\n",
              "      <td>0.208333</td>\n",
              "      <td>0.042660</td>\n",
              "      <td>0.002395</td>\n",
              "      <td>0.356424</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.085271</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.320755</td>\n",
              "      <td>0.297872</td>\n",
              "      <td>0.333936</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.099448</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.445355</td>\n",
              "      <td>0.202479</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.035968</td>\n",
              "      <td>0.000952</td>\n",
              "      <td>0.279348</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.275927</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.117021</td>\n",
              "      <td>0.207136</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.259669</td>\n",
              "      <td>0.775</td>\n",
              "      <td>0.150273</td>\n",
              "      <td>0.264463</td>\n",
              "      <td>0.152778</td>\n",
              "      <td>0.025512</td>\n",
              "      <td>0.005485</td>\n",
              "      <td>0.410440</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8741</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.113871</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.415094</td>\n",
              "      <td>0.202128</td>\n",
              "      <td>0.378418</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.080110</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.349727</td>\n",
              "      <td>0.190083</td>\n",
              "      <td>0.472222</td>\n",
              "      <td>0.021330</td>\n",
              "      <td>0.002275</td>\n",
              "      <td>0.240833</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8742</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.085501</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.295597</td>\n",
              "      <td>0.351064</td>\n",
              "      <td>0.046289</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.458564</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.202186</td>\n",
              "      <td>0.161157</td>\n",
              "      <td>0.243056</td>\n",
              "      <td>0.030113</td>\n",
              "      <td>0.087828</td>\n",
              "      <td>0.509474</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8743</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.281475</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.408805</td>\n",
              "      <td>0.372340</td>\n",
              "      <td>0.350259</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.160221</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.262295</td>\n",
              "      <td>0.148760</td>\n",
              "      <td>0.201389</td>\n",
              "      <td>0.021748</td>\n",
              "      <td>0.005113</td>\n",
              "      <td>0.102543</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8744</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.233105</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.515723</td>\n",
              "      <td>0.563830</td>\n",
              "      <td>0.102956</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.337017</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.475410</td>\n",
              "      <td>0.243802</td>\n",
              "      <td>0.138889</td>\n",
              "      <td>0.144709</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.391090</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8745</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.070547</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.465409</td>\n",
              "      <td>0.436170</td>\n",
              "      <td>0.451350</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.060773</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.142077</td>\n",
              "      <td>0.177686</td>\n",
              "      <td>0.243056</td>\n",
              "      <td>0.020075</td>\n",
              "      <td>0.001562</td>\n",
              "      <td>0.305221</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8746 rows Ã— 114 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      INTENSIVE  INCLUSIONFRS  NOAGENTS  ...    UMALCR       BMI  SBPTERTILE\n",
              "0             1             1         0  ...  0.000888  0.346973         1.0\n",
              "1             1             1         0  ...  0.000818  0.270478         0.5\n",
              "2             0             1         0  ...  0.002395  0.356424         0.5\n",
              "3             1             0         0  ...  0.000952  0.279348         0.0\n",
              "4             0             1         0  ...  0.005485  0.410440         0.0\n",
              "...         ...           ...       ...  ...       ...       ...         ...\n",
              "8741          1             0         0  ...  0.002275  0.240833         0.5\n",
              "8742          1             0         0  ...  0.087828  0.509474         0.0\n",
              "8743          1             1         1  ...  0.005113  0.102543         0.5\n",
              "8744          0             1         1  ...  0.000368  0.391090         1.0\n",
              "8745          0             0         0  ...  0.001562  0.305221         1.0\n",
              "\n",
              "[8746 rows x 114 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "outcome: EVENT_PRIMARY\n",
            "predictors: ['INTENSIVE', 'INCLUSIONFRS', 'NOAGENTS', 'ASPIRIN', 'SUB_CKD', 'RACE_BLACK', 'FEMALE', 'SUB_CVD', 'SUB_CLINICALCVD', 'SUB_SUBCLINICALCVD', 'SUB_SENIOR', 'STATIN', 'NEWSITEID_1.0', 'NEWSITEID_2.0', 'NEWSITEID_3.0', 'NEWSITEID_4.0', 'NEWSITEID_5.0', 'NEWSITEID_6.0', 'NEWSITEID_8.0', 'NEWSITEID_9.0', 'NEWSITEID_10.0', 'NEWSITEID_11.0', 'NEWSITEID_12.0', 'NEWSITEID_13.0', 'NEWSITEID_15.0', 'NEWSITEID_18.0', 'NEWSITEID_19.0', 'NEWSITEID_20.0', 'NEWSITEID_21.0', 'NEWSITEID_22.0', 'NEWSITEID_23.0', 'NEWSITEID_24.0', 'NEWSITEID_25.0', 'NEWSITEID_27.0', 'NEWSITEID_29.0', 'NEWSITEID_31.0', 'NEWSITEID_32.0', 'NEWSITEID_33.0', 'NEWSITEID_36.0', 'NEWSITEID_37.0', 'NEWSITEID_38.0', 'NEWSITEID_40.0', 'NEWSITEID_41.0', 'NEWSITEID_42.0', 'NEWSITEID_43.0', 'NEWSITEID_44.0', 'NEWSITEID_45.0', 'NEWSITEID_47.0', 'NEWSITEID_48.0', 'NEWSITEID_50.0', 'NEWSITEID_51.0', 'NEWSITEID_52.0', 'NEWSITEID_54.0', 'NEWSITEID_55.0', 'NEWSITEID_56.0', 'NEWSITEID_57.0', 'NEWSITEID_58.0', 'NEWSITEID_59.0', 'NEWSITEID_60.0', 'NEWSITEID_61.0', 'NEWSITEID_62.0', 'NEWSITEID_63.0', 'NEWSITEID_64.0', 'NEWSITEID_65.0', 'NEWSITEID_66.0', 'NEWSITEID_67.0', 'NEWSITEID_68.0', 'NEWSITEID_70.0', 'NEWSITEID_71.0', 'NEWSITEID_72.0', 'NEWSITEID_74.0', 'NEWSITEID_75.0', 'NEWSITEID_76.0', 'NEWSITEID_77.0', 'NEWSITEID_79.0', 'NEWSITEID_80.0', 'NEWSITEID_81.0', 'NEWSITEID_82.0', 'NEWSITEID_83.0', 'NEWSITEID_85.0', 'NEWSITEID_87.0', 'NEWSITEID_88.0', 'NEWSITEID_89.0', 'NEWSITEID_90.0', 'NEWSITEID_91.0', 'NEWSITEID_92.0', 'NEWSITEID_94.0', 'NEWSITEID_95.0', 'NEWSITEID_96.0', 'NEWSITEID_97.0', 'NEWSITEID_98.0', 'NEWSITEID_99.0', 'NEWSITEID_100.0', 'NEWSITEID_102.0', 'RACE4_BLACK', 'RACE4_HISPANIC', 'RACE4_OTHER', 'RACE4_WHITE', 'RISK10YRS', 'N_AGENTS', 'SBP', 'DBP', 'EGFR', 'SMOKE_3CAT', 'SCREAT', 'AGE', 'CHR', 'GLUR', 'HDL', 'TRR', 'UMALCR', 'BMI', 'SBPTERTILE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjhEYSJ1EUJe"
      },
      "source": [
        "# **Models training**\n",
        "\n",
        "First we split the data into two main sets - train and test.\n",
        "\n",
        " We train three models using **cross-validation and grid search** to optimize their performances. The chosen metric is **AUC rather then accuracy** to overcome the fact that the data is heavily imbalanced. Beacause the dataset is imbalanced we will also use **over and under sampling** methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be7qZBgAVKSP"
      },
      "source": [
        "# train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(processed_df[predictors], processed_df[outcome], test_size=1/3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LKMDVKWzIBj",
        "outputId": "ad051edd-3f9d-4b07-fec4-67db20ac594f"
      },
      "source": [
        "from collections import Counter\n",
        "Counter(y_train)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 5467, 1: 363})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nODJ5Nv2EX3e"
      },
      "source": [
        "# **Linear model**\n",
        "\n",
        "We train a **Linear SVM** model. \n",
        "\n",
        "We chose this model and fine tuined the hyper-paraetrs of it using grid search 5-fold cross validation. \n",
        "\n",
        "In addition, beacause the dataset is imbalanced, we also searched over different over and under sampling configurations. For both over and under sampling methods we added a search over the option of not using each method (*lambda\\ y: Counter(y)*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czASSDRwa7i3",
        "outputId": "90f9a85e-1e7d-4471-a300-8630ee632e35"
      },
      "source": [
        "linear_model = LinearSVC()\n",
        "# LinearSVC(tol=1e-10, loss='hinge', C=1000, max_iter=50000)\n",
        "parameters = {\n",
        "              'm__loss': ['hinge', 'squared_hinge'], \n",
        "              'm__C': [0.001,0.01,0.1,1,10,100,1000],\n",
        "              'm__tol':[1e-10, 1e-6, 1e-4],\n",
        "              'm__max_iter':[1000,10000, 50000],\n",
        "              'o__sampling_strategy':[lambda y: Counter(y), 0.5, 1],\n",
        "              'u__sampling_strategy':[lambda y: Counter(y), 1]\n",
        "             }\n",
        "\n",
        "roc_auc_scorer = make_scorer(roc_auc_score)\n",
        "\n",
        "\n",
        "over_sampler = SMOTE()\n",
        "under_sampler = RandomUnderSampler()\n",
        "\n",
        "linear_pipeline = Pipeline(steps=[('o', over_sampler), ('u', under_sampler), ('m', linear_model)])\n",
        "\n",
        "\n",
        "# Run the grid search\n",
        "Linear_grid_obj = GridSearchCV(linear_pipeline, parameters,  scoring=roc_auc_scorer, verbose=1) # 5-folds cv\n",
        "Linear_grid_obj = Linear_grid_obj.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "print(Linear_grid_obj.best_params_)\n",
        "print(Linear_grid_obj.best_score_)\n",
        "Linear_best_params = Linear_grid_obj.best_params_\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'m__C': 0.01, 'm__loss': 'squared_hinge', 'm__max_iter': 50000, 'm__tol': 1e-10, 'o__sampling_strategy': <function <lambda> at 0x7f804796b620>, 'u__sampling_strategy': 1}\n",
            "0.6458087367178276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRxZCBckEYNk"
      },
      "source": [
        "# **Ensemble model**\n",
        "\n",
        "We chose to train **Random Forest Classifier**. \n",
        "\n",
        "We chose this model for it's one of the most widely used ensemble learning model. As with the linear model we fine tuined the hyper-paraetrs of it using grid search 5-fold cross validation. \n",
        "\n",
        "As before we also searched over different over and under sampling configurations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5VVA4MFa8LH",
        "outputId": "4cc4f45b-0ee2-4b11-e3c3-a2082c43afd4"
      },
      "source": [
        "ensemble_model = RandomForestClassifier(bootstrap=True)\n",
        "\n",
        "\n",
        "parameters = {'m__n_estimators': [25, 99, 199],\n",
        "              'm__max_features': ['log2', 'sqrt','auto'], \n",
        "              'm__criterion': ['entropy', 'gini'],\n",
        "              'm__max_depth': [2, 5, 7, None], \n",
        "              'm__min_samples_split': [2, 4,10],\n",
        "              'm__min_samples_leaf': [1,2,5],\n",
        "              'm__ccp_alpha': [0.0, 0.01, 0.05],\n",
        "              'o__sampling_strategy':[lambda y: Counter(y), 0.5, 1],\n",
        "              'u__sampling_strategy':[lambda y: Counter(y), 1]\n",
        "             }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "roc_auc_scorer = make_scorer(roc_auc_score)\n",
        "\n",
        "over_sampler = SMOTE()\n",
        "under_sampler = RandomUnderSampler()\n",
        "\n",
        "ensemble_pipeline = Pipeline(steps=[('o', over_sampler), ('u', under_sampler), ('m', ensemble_model)])\n",
        "\n",
        "# Run the grid search\n",
        "ensemble_grid_obj = GridSearchCV(ensemble_pipeline, parameters, scoring=roc_auc_scorer, n_jobs=-1, verbose=4) # 5-folds cv\n",
        "ensemble_grid_obj = ensemble_grid_obj.fit(x_train, y_train)\n",
        "\n",
        "print(ensemble_grid_obj.best_params_)\n",
        "print(ensemble_grid_obj.best_score_)\n",
        "ensemble_best_params = ensemble_grid_obj.best_params_\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'m__ccp_alpha': 0.01, 'm__criterion': 'entropy', 'm__max_depth': 7, 'm__max_features': 'log2', 'm__min_samples_leaf': 5, 'm__min_samples_split': 4, 'm__n_estimators': 199, 'o__sampling_strategy': <function <lambda> at 0x7f80479e02f0>, 'u__sampling_strategy': 1}\n",
            "0.6639118457300276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEhGLOAHEYVD"
      },
      "source": [
        "# **Deep Learning model**\n",
        "\n",
        "We train multiple different **Fully Connected Feed-Forward Neural Networks** models with several hidden layers. \n",
        "Our shallowest network cossists of a single hidden layer, while our deepest consists of four.\n",
        "\n",
        "We defined a function (*baseline_model*) to initializie and compile a NN model according to the chosen architecture of each configuration. \n",
        "\n",
        "We then run a 5-fold cross validation functuion (cv_score) to estimate the AUC score of each model. finally, we grid-searched over a set of pre-defined chosen hyper-parameters and architectures to establish the best combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzT9eCcha9DY",
        "outputId": "5283a811-c1d0-45dd-cb76-a8a035f4bb89"
      },
      "source": [
        "# 4 - define baseline model\n",
        "def baseline_model(layers, input_shape, lr):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    # 5 - lower number of neurons than the unput vector size\n",
        "    model.add(Dense(layers[0]['units'], input_dim=input_shape, kernel_initializer='normal', activation=layers[0]['act']))\n",
        "\n",
        "    for layer in layers[1:]:\n",
        "      model.add(Dense(layer['units'], kernel_initializer='normal', activation=layer['act']))\n",
        "\n",
        "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))    # Compile model\n",
        "    opt = Adam(learning_rate=lr)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "layers = [[{'units':100,'act':'relu'},{'units':50,'act':'relu'},{'units':20,'act':'relu'}],\n",
        "          [{'units':100,'act':'relu'},{'units':20,'act':'relu'},{'units':10,'act':'relu'}],\n",
        "          [{'units':100,'act':'relu'},{'units':50,'act':'relu'}],\n",
        "          [{'units':100,'act':'relu'},{'units':20,'act':'relu'}],\n",
        "          [{'units':50,'act':'relu'},{'units':20,'act':'relu'}],\n",
        "          [{'units':50,'act':'relu'}]]\n",
        "\n",
        "parameters = {'layers': layers ,\n",
        "              'epochs': [10,20,30], \n",
        "              'batch_size': [8,16,32],\n",
        "              'learning_rate': [0.0001,0.001,0.01],\n",
        "              'o__sampling_strategy':[1, 0.5,lambda y: Counter(y)],\n",
        "              'u__sampling_strategy':[1, lambda y: Counter(y)]\n",
        "             }\n",
        "\n",
        "def cv_score(parameters, x, y):\n",
        "    kf = KFold(n_splits=5, shuffle=True)\n",
        "    auc_scores = []\n",
        "    m = AUC()\n",
        "    over_sampler = SMOTE(parameters['over_samp'])\n",
        "    under_sampler = RandomUnderSampler(parameters['under_samp'])\n",
        "\n",
        "    for train_index, val_index in kf.split(x):\n",
        "      model = baseline_model(parameters['model_layout'], 113,parameters['lr'] )\n",
        "\n",
        "      x_train_cv, x_val_cv = x[train_index], x[val_index]\n",
        "      y_train_cv, y_val_cv = y[train_index], y[val_index]\n",
        "      \n",
        "      x_train_cv, y_train_cv = over_sampler.fit_resample(x_train_cv,y_train_cv)\n",
        "      x_train_cv, y_train_cv = under_sampler.fit_resample(x_train_cv,y_train_cv)\n",
        "\n",
        "      # Training the algorithm using the predictors and target.\n",
        "      model.fit(x_train_cv, y_train_cv, epochs=parameters['epoch'], batch_size=parameters['batch'], verbose=0) \n",
        "      # Record accuracy from each cross-validation run\n",
        "      predicts = model.predict(x_val_cv, verbose=0)\n",
        "      m.reset_states()\n",
        "      m.update_state(y_val_cv, predicts)\n",
        "      auc_score = m.result().numpy()\n",
        "      auc_scores.append(auc_score)\n",
        "\n",
        "    mean_auc_cv_score = np.mean(auc_scores)\n",
        "    print(f\"Cross-Validation auc_score {np.mean(auc_scores)}\")\n",
        "    return mean_auc_cv_score\n",
        "\n",
        "DL_max_auc = 0\n",
        "DL_best_params = []\n",
        "for model_layout in parameters['layers']:\n",
        "  for epoch in parameters['epochs']:\n",
        "    for batch in parameters['batch_size']:\n",
        "      for lr in parameters['learning_rate']:\n",
        "        for o_samp_strat in parameters['o__sampling_strategy']:\n",
        "          for u_samp_strat in parameters['u__sampling_strategy']:\n",
        "            params = {'model_layout':model_layout, \"epoch\":epoch, \"batch\":batch, 'lr':lr, 'under_samp':u_samp_strat, 'over_samp':o_samp_strat}\n",
        "            print(params)\n",
        "            mean_auc = cv_score(params, x_train.values, y_train.values)\n",
        "            print('---')\n",
        "            if mean_auc > DL_max_auc:\n",
        "              DL_best_params = params\n",
        "              DL_max_auc = mean_auc\n",
        "\n",
        "print(f\"DL_max_auc: {DL_max_auc}\")\n",
        "print(f\"best_params: {DL_best_params}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DL_max_auc: 0.6747558116912842\n",
            "best_params: {'model_layout': [{'units': 50, 'act': 'relu'}], 'epoch': 10, 'batch': 16, 'lr': 0.001, 'under_samp': 1, 'over_samp': <function <lambda> at 0x7f8047960c80>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX1bKRO58Eim"
      },
      "source": [
        "# **Models comparison**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeOi8cof8Su4",
        "outputId": "1dc2bc57-6937-4ac1-be0b-5f118f6ac5ec"
      },
      "source": [
        "warnings.simplefilter(action=\"default\", category=FutureWarning)\n",
        "\n",
        "# train the best linear model\n",
        "linear_best_model = LinearSVC()\n",
        "linear_best_pipeline = Pipeline(steps=[('o',  SMOTE()), ('u', RandomUnderSampler()), ('m', linear_best_model)])\n",
        "linear_best_pipeline.set_params(**Linear_best_params)\n",
        "linear_best_pipeline.fit(x_train, y_train)\n",
        "# train the best enesemble model\n",
        "ensemble_best_model = RandomForestClassifier(bootstrap=True)\n",
        "ensemble_best_pipeline = Pipeline(steps=[('o',  SMOTE()), ('u', RandomUnderSampler()), ('m', ensemble_best_model)])\n",
        "ensemble_best_pipeline.set_params(**ensemble_best_params)\n",
        "ensemble_best_pipeline.fit(x_train, y_train)\n",
        "# train the best Deep NN model\n",
        "over_sampler = SMOTE(DL_best_params['over_samp'])\n",
        "under_sampler = RandomUnderSampler(DL_best_params['under_samp'])\n",
        "x_train_resamp, y_train_resamp = over_sampler.fit_resample(x_train, y_train)\n",
        "x_train_resamp, y_train_resamp = under_sampler.fit_resample(x_train_resamp, y_train_resamp)\n",
        "\n",
        "DL_best_model = baseline_model(DL_best_params['model_layout'], 113, DL_best_params['lr'] )\n",
        "DL_best_model.fit(x_train_resamp, y_train_resamp, epochs=DL_best_params['epoch'], batch_size=DL_best_params['batch'], verbose=0) \n",
        "\n",
        "#generate predictions on the test set\n",
        "linear_predicts = linear_best_pipeline.predict(x_test)\n",
        "ensemble_predicts = ensemble_best_pipeline.predict(x_test)\n",
        "DL_predicts = DL_best_model.predict(x_test, verbose=0)\n",
        "\n",
        "print(\"\\nLinear model results\")\n",
        "print('Final Linear model AUC: ', roc_auc_score(y_test, linear_predicts))\n",
        "print(classification_report(y_test, linear_predicts))\n",
        "\n",
        "print(\"\\nEnsemble model results\")\n",
        "print('Final Ensemble model AUC: ', roc_auc_score(y_test, ensemble_predicts))\n",
        "print(classification_report(y_test, ensemble_predicts))\n",
        "\n",
        "print(\"\\nDL model results\")\n",
        "print('Final DL model AUC: ', roc_auc_score(y_test, DL_predicts))\n",
        "DL_predicts = [0 if i<0.5 else 1 for i in DL_predicts]\n",
        "print(classification_report(y_test, DL_predicts))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Linear model results\n",
            "Final Linear model AUC:  0.6259836937256291\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.65      0.78      2730\n",
            "           1       0.10      0.60      0.18       186\n",
            "\n",
            "    accuracy                           0.65      2916\n",
            "   macro avg       0.53      0.63      0.48      2916\n",
            "weighted avg       0.91      0.65      0.74      2916\n",
            "\n",
            "\n",
            "Ensemble model results\n",
            "Final Ensemble model AUC:  0.6419945645752096\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.64      0.77      2730\n",
            "           1       0.11      0.65      0.19       186\n",
            "\n",
            "    accuracy                           0.64      2916\n",
            "   macro avg       0.54      0.64      0.48      2916\n",
            "weighted avg       0.91      0.64      0.73      2916\n",
            "\n",
            "\n",
            "DL model results\n",
            "Final DL model AUC:  0.66794674859191\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.63      0.76      2730\n",
            "           1       0.10      0.63      0.18       186\n",
            "\n",
            "    accuracy                           0.63      2916\n",
            "   macro avg       0.53      0.63      0.47      2916\n",
            "weighted avg       0.91      0.63      0.73      2916\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "TtfzuDbmYaqR",
        "outputId": "553b260a-1c10-4cfe-a95b-2fe2727fee2d"
      },
      "source": [
        "print(\"Linear model\")\n",
        "liear_cf = confusion_matrix(y_test, linear_predicts)\n",
        "ConfusionMatrixDisplay(liear_cf, display_labels=[0,1]).plot(values_format='d')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f80338a2d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "ObJfN7kmbfZs",
        "outputId": "7d74d6e9-2f24-4b60-f201-b94f19cc48d1"
      },
      "source": [
        "print(\"Ensemble model\")\n",
        "ensemble_cf = confusion_matrix(y_test, ensemble_predicts)\n",
        "ConfusionMatrixDisplay(ensemble_cf, display_labels=[0,1]).plot(values_format='d')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ensemble model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f8033a5a208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "lqDXbmlibhNO",
        "outputId": "e1c03f9f-5613-460f-e43e-231ea64f7985"
      },
      "source": [
        "\n",
        "print(\"DL model\")\n",
        "DL_cf = confusion_matrix(y_test, DL_predicts)\n",
        "ConfusionMatrixDisplay(DL_cf, display_labels=[0,1]).plot(values_format='d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DL model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f8033ad6780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWCm-Ub6LscK"
      },
      "source": [
        "# **Summary and Conclusion** \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s9L7tvYBmGn"
      },
      "source": [
        "### **Metric selection** \n",
        "\n",
        "\n",
        "We chose the metric of ***AUC*** rather then *accuracy* to overcome the fact that the data is heavily imbalanced. At first we considered using *accuracy* as the comparison metric, but evan a model based on a majority rule will achive a high *accuracy* score ( always predicts 0 - no cardiovascular event). \n",
        "\n",
        "We also considered using the ***Recall*** metric because a high *recall* score will indicate that a high percentage of sick patients will be identified. despite the fact that this metric seems to fit this problem well enougth we chose *AUC* instead. \n",
        "\n",
        "*AUC* takes into consideration *TPR* and *FPR* while *Percision* (or *sensitivity*) regards only *TPR*. It is our understanding that there is not only a risk in not identifing a sick patient, but there is also a significant risk in identifing a healthy one as sick. A false identification (i.e. *False Positive*) can lead to the wrong treatment and have severe complications. Therefore, we chose the *AUC* score metric.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etSFOX8YBz_K"
      },
      "source": [
        "### **Quantitative results discussion**\n",
        "\n",
        "\n",
        "As seen by the final results The DL model is proved to perform at the highest level for this task out of the examined models. In addition the ensemble model performed slightly better than the linear model did. As explained above the fact that **the DL model achived the highest *AUC* score means that it's the best model, in our opinion, for this task.**\n",
        "\n",
        "### **Qualitive results discussion**\n",
        "\n",
        "We calculated the confusion matrix to better visualize the predictions made by each model.\n",
        "\n",
        "**The actual desicion of which model to use should be affected by the prefrences of the user.** For example, if the user wants a low false negative rate they should use ensemble model. In contrast if they seek a low false positive rate they should chose the linear model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoYpw7nMCAPT"
      },
      "source": [
        "# **Methods summary**\n",
        "\n",
        "### **Linear model: Linear SVC**\n",
        "\n",
        "**Parameters chosen:**\n",
        "\n",
        "C: 0.01, loss: 'squared_hinge', max_iter: 50000, tol: 1e-10, no oversampling and full undersampling (same number of samples for each label)\n",
        "\n",
        "\n",
        "### **Ensemble model: Random Forest Classifier**\n",
        "\n",
        "**Parameters chosen:**\n",
        "\n",
        "ccp_alpha: 0.01, criterion: 'entropy', max_depth: 7, max_features: 'log2', min_samples_leaf: 5, min_samples_split: 4, n_estimators: 199, no oversampling and full undersampling (same number of samples for each label)\n",
        "\n",
        "\n",
        "### **Deep Learning model: Fully Connected Neural Network**\n",
        "\n",
        "Architecture chosen: One hidden layer with 50 FC units using ReLU activation function. Sigmoid activation for the output layer. binary_crossentropy loss was used as well as Adam optimaizer.\n",
        "\n",
        "**Parameters chosen:**\n",
        "\n",
        "epoch: 10, batch: 16, learning rate: 0.001, no oversampling and full undersampling (same number of samples for each label)"
      ]
    }
  ]
}